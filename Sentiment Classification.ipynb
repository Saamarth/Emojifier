{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87c4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a568d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Embeddings from 'glove.6B.50d.txt' file\n",
    "\n",
    "f=open(\"glove.6B.50d.txt\", \"r\", encoding='utf8')\n",
    "word_to_vec_map={}\n",
    "words=set()\n",
    "\n",
    "for line in f:\n",
    "    line_list=line.strip().split()\n",
    "    words.add(line_list[0])\n",
    "    word_to_vec_map[line_list[0]]=np.array( line_list[1:] , dtype=np.float)\n",
    "index_to_word={k:v for k,v in enumerate(sorted(words), start=1)}\n",
    "word_to_index={v:k for k,v in enumerate(sorted(words), start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e867f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab is of size : 400000\n",
      "Size of Embeddings : (50,)\n"
     ]
    }
   ],
   "source": [
    "#Analysing the Vocabulary.\n",
    "vocab_size=len(word_to_vec_map.keys())\n",
    "print(\"Vocab is of size : {}\".format(vocab_size))\n",
    "\n",
    "Embeddings_size=word_to_vec_map['father'].shape\n",
    "print(\"Size of Embeddings : {}\".format(Embeddings_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "206b0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data from the files.\n",
    "df_train=pd.read_csv('./data/train_emoji.csv')\n",
    "df_test=pd.read_csv(\"./data/test_emoji.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296283f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (132,)\n",
      "Shape of y_train: (132,)\n",
      "Shape of x_test: (56,)\n",
      "Shape of y_tesr: (56,)\n"
     ]
    }
   ],
   "source": [
    "#Analysing the data\n",
    "x_train=np.array(df_train['sentence'])\n",
    "y_train=np.array(df_train['label'])\n",
    "\n",
    "x_test=np.array(df_test['sentence'])\n",
    "y_test=np.array(df_test['label'])\n",
    "\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
    "print(\"Shape of y_tesr: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3125e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {0: \"\\u2764\\uFE0F\",    # heart \n",
    "                    1: \":baseball:\",\n",
    "                    2: \":smile:\",\n",
    "                    3: \":disappointed:\",\n",
    "                    4: \":fork_and_knife:\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    This takes the label from {0,1,2,3,4} and returns the corresponding emoji.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_code=emoji_dictionary[label]\n",
    "    a=emoji.emojize(label_code, use_aliases=True)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29796be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again 3 üòû\n",
      "I am proud of your achievements 2 üòÑ\n",
      "It is the worst day in my life 3 üòû\n",
      "Miss you so much 0 ‚ù§Ô∏è\n",
      "food is life 4 üç¥\n",
      "I love you mum 0 ‚ù§Ô∏è\n",
      "Stop saying bullshit 3 üòû\n",
      "congratulations on your acceptance 2 üòÑ\n",
      "The assignment is too long  3 üòû\n",
      "I want to go play 1 ‚öæ\n"
     ]
    }
   ],
   "source": [
    "#Getting the glimse of the data\n",
    "for i in range(10):\n",
    "    print(x_train[i], y_train[i], label_to_emoji(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2997b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's convert the labels into one-hot encodings.\n",
    "\n",
    "def labels_to_oh(Y, classes=5):\n",
    "    \"\"\"\n",
    "    This function converts the labels into one-hot encodings.\n",
    "    Arguments:\n",
    "        Y: Array of shape(m,)\n",
    "        classes: int\n",
    "    Returns:\n",
    "        one_hot: Array of shape(classes, m)\n",
    "    \"\"\"\n",
    "    one_hot=np.zeros((classes,Y.shape[0]))\n",
    "    m_list=np.arange(Y.shape[0])\n",
    "    one_hot[Y,m_list ]=1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e97c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'You are incredibly intelligent and talented' has label index 2  which is emoji üòÑ\n",
      "Label index 2 in one-hot encoding format is: \n",
      " [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#Let's check the above function:\n",
    "idx = 59  #Feel free to play with this\n",
    "print(\"Sentence '{}' has label index {}  which is emoji {}\".format( x_train[idx], y_train[idx], label_to_emoji(y_train[idx]) ) )\n",
    "print(\"Label index {} in one-hot encoding format is: \\n {}\".format( y_train[idx], labels_to_oh(np.array([ y_train[idx] ]), 5) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c28474",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "775aca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_fun(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a single sentence and returns the average vector of it's words embeddings.\n",
    "    Arguments: \n",
    "        sentence: a string of single sentence\n",
    "    \n",
    "    Returns:\n",
    "        arr: A average vector , of shape(50,)\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_list=sentence.split()\n",
    "    arr=np.zeros( word_to_vec_map['father'].shape )\n",
    "    for i in range( len(sentence_list) ):\n",
    "        arr+=word_to_vec_map[sentence_list[i].lower()]\n",
    "    arr=arr/len(sentence_list)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe524952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-0.5081    -0.49981    0.910065  -0.417435  -0.0855345  0.1721485\n",
      " -0.221605  -0.2937245 -0.570465  -0.02338   -0.306108  -0.84905\n",
      " -0.201511   0.63919   -0.91403   -0.281875  -0.403532   0.366115\n",
      "  0.089415  -0.633365   0.0429     0.145705   0.0418234  0.16978\n",
      "  0.1502575 -1.2991    -0.61849    0.47496    0.34654   -0.40018\n",
      "  2.1238    -0.3645935 -0.566265  -0.021425  -0.695707   0.02204\n",
      "  0.05728    0.4932     0.199505   0.049545   0.5472715  0.075176\n",
      " -0.539305   0.467965   0.079155  -0.357905  -0.10787   -0.36209\n",
      "  0.44183   -0.081715 ]\n"
     ]
    }
   ],
   "source": [
    "# checking the above function\n",
    "avg = avg_fun(\"Machine Leaning\")\n",
    "print(\"avg = \\n\", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2c97093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our usual softmax\n",
    "def softmax(x):\n",
    "    temp=np.exp(x)\n",
    "    return temp/np.sum(temp, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "432153a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now comes the first real game\n",
    "\n",
    "def model(x_train, y_train, learning_rate=0.01, num_of_iters=80000):\n",
    "    \"\"\"\n",
    "    This function trains our model for sentiment-classification using avg_fun function and neural-network\n",
    "    Arguments:\n",
    "        x_train: X-training-set , having shape (m,) .Here ,m=132\n",
    "        y_train: y-training-set , having shape (m,)\n",
    "    \n",
    "    Returns:\n",
    "        W: Parameter of the model, having shape (5,50)\n",
    "        b: Parameter of the model, having shape (5,1)\n",
    "        loss_list: List containing the loss over consecutive epochs, useful for finding num_of_iters\n",
    "    \"\"\"\n",
    "    \n",
    "    y_train_final=labels_to_oh(y_train) # y_train_final : shape=(5,132)\n",
    "   \n",
    "    \n",
    "    x_train_final=np.array( [avg_fun(i) for i in x_train] ).T  # x_train_final:shape(50,132)\n",
    "    m=x_train_final.shape[1] # no. of training examples\n",
    "    \n",
    "    n_0=x_train_final.shape[0]\n",
    "    n_1=y_train_final.shape[0]\n",
    "    \n",
    "    #Initializing my parameters\n",
    "    W=np.random.randn(n_1,n_0) / np.sqrt(n_0)\n",
    "    b=np.zeros( (n_1,1) )\n",
    "    \n",
    "    loss_list=[]\n",
    "    for epoch in range( num_of_iters):\n",
    "        \n",
    "        #forward Pass\n",
    "        A=softmax(np.dot(W, x_train_final)+b)\n",
    "        \n",
    "        loss=-np.sum( ( y_train_final*np.log(A) ))\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        #Backward Pass\n",
    "        dZ=A-y_train_final\n",
    "        dW=( np.dot(dZ, x_train_final.T) ) /m\n",
    "        db=( np.sum( dZ, axis=1, keepdims=True ) ) /m\n",
    "        \n",
    "        #updating the parameters\n",
    "        W=W-learning_rate*dW        \n",
    "        b=b-learning_rate*db\n",
    "        \n",
    "    return W,b, loss_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "740b2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b, loss_list =model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f45aea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96112f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojifier_model1(sentence, W,b):\n",
    "    \"\"\"\n",
    "    This function returns the appropriate emoji to the given sentence.\n",
    "    Arguments:\n",
    "        sentence: A string\n",
    "        W: Trained parameter of the above model\n",
    "        b: Trained parameter of the above model\n",
    "    \n",
    "    Returns:\n",
    "        emo : A emoji\n",
    "    \"\"\"\n",
    "    avg_vec=avg_fun(sentence)\n",
    "    avg_vec=avg_vec.reshape(50,1)\n",
    "    A = softmax(np.dot(W, avg_vec)+b)\n",
    "    \n",
    "    label=np.argmax(A)\n",
    "    \n",
    "    emo=label_to_emoji(label)\n",
    "    print(sentence, \"--->\", emo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b582b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am in love ---> üòÑ\n",
      "i love you ---> ‚ù§Ô∏è\n",
      "I play ---> ‚öæ\n",
      "i am in good mood ---> üòÑ\n",
      "i am not in good mood ---> üòÑ\n"
     ]
    }
   ],
   "source": [
    "emojifier_model1(\"i am in love\", W, b) \n",
    "emojifier_model1(\"i love you\", W, b) \n",
    "emojifier_model1(\"I play\", W, b) \n",
    "emojifier_model1(\"i am in good mood\", W, b) \n",
    "emojifier_model1(\"i am not in good mood\", W, b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a94dcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_analysis( X, Y, W, b, word_to_vec_map ):\n",
    "    \"\"\"\n",
    "    Given the X set(Arr of sentences) and Y set(Arr of Labels) , this computes the prediction over \n",
    "    all the set and calculates the accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_final=np.array( [avg_fun(i) for i in X] ).T #shape(50,m)\n",
    "    m=X.shape[0]\n",
    "    \n",
    "    A=softmax(np.dot(W, X_final)+b)\n",
    "    pred=np.argmax(A, axis=0)\n",
    "    \n",
    "    print(\"Accuracy: \"  + str(np.mean( (pred[:] == Y[:]) )))\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f92f846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.9848484848484849\n",
      "----------------\n",
      "Test set:\n",
      "Accuracy: 0.8928571428571429\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = prediction_analysis(x_train, y_train, W, b, word_to_vec_map)\n",
    "print(\"----------------\")\n",
    "print('Test set:')\n",
    "pred_test = prediction_analysis(x_test, y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444da098",
   "metadata": {},
   "source": [
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7564d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a02d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the maximum length for setting T_x in the model.\n",
    "max_len= len( max(x_train, key=len).split() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a13af754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indicies(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    This function takes the sentences and converts them into array of indices.\n",
    "    Arguments:\n",
    "        X: Dataset, of size(m,)\n",
    "        word_to_index: Dictionary of word ---> index\n",
    "        max_len: Int, having value equal to max. length of sentence out of sentences. \n",
    "        \n",
    "    Returns:\n",
    "        mat: Array of indices, shape:(m,max_len)\n",
    "1    \"\"\" \n",
    "    \n",
    "    m=X.shape[0]\n",
    "    \n",
    "    mat=np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        \n",
    "        sentence_list= X[i].lower().split()\n",
    "        for j in range( len(sentence_list) ):\n",
    "            mat[i,j]=word_to_index[sentence_list[j]]\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12f58636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    This function returns the embedding layer with our Glove embeddings set.\n",
    "    Arguments:\n",
    "        word_to_vec_map: Dictionary\n",
    "        word_to_index: Dictionary\n",
    "    \n",
    "    Returns:\n",
    "        embedding_layer\n",
    "    \"\"\"\n",
    "    vocab_size=len(word_to_index.keys())+1\n",
    "    emb_size=word_to_vec_map['mango'].shape[0]\n",
    "    \n",
    "    emb_matrix=np.zeros((vocab_size, emb_size))\n",
    "    \n",
    "    for word, idx in word_to_index.items() :\n",
    "        emb_matrix[idx,:]=word_to_vec_map[word]\n",
    "    \n",
    "    embedding_layer=tf.keras.layers.Embedding(\n",
    "                                        input_dim=vocab_size,\n",
    "                                        output_dim=emb_size,\n",
    "                                        trainable=False\n",
    "                                            )\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7b0607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, let's start building the model\n",
    "def build_model(input_shape,word_to_vec_map, word_to_index ):\n",
    "    i=tf.keras.Input(\n",
    "        shape=input_shape\n",
    "        )\n",
    "    x=pretrained_embedding_layer(word_to_vec_map, word_to_index)(i)\n",
    "    x=tf.keras.layers.LSTM(\n",
    "        units=128,\n",
    "        return_sequences=True,\n",
    "        )(x)\n",
    "    x=tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "    x=tf.keras.layers.LSTM(\n",
    "        units=128,\n",
    "        return_sequences=False,\n",
    "        )(x)\n",
    "    x=tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "    o=tf.keras.layers.Dense(\n",
    "        units=5,\n",
    "        activation='softmax',\n",
    "        use_bias=False,\n",
    "        )(x)\n",
    "    \n",
    "    model=tf.keras.Model(inputs=i, outputs=o)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a20cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 640       \n",
      "=================================================================\n",
      "Total params: 20,223,922\n",
      "Trainable params: 223,872\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape =(max_len) \n",
    "model=build_model(input_shape,word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a92ebcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "los=tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    #label_smoothing=0.0,\n",
    "    #axis=-1,\n",
    "    #reduction=\"auto\",\n",
    "    #name=\"categorical_crossentropy\"\n",
    "    )\n",
    "met=['accuracy']    \n",
    "model.compile(\n",
    "    optimizer='adam', loss=los, metrics=met\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a30284f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparation of data\n",
    "x_final=sentences_to_indicies(x_train, word_to_index, max_len) #shape=(m,max_len=10)\n",
    "y_final=labels_to_oh(y_train) # here, shape=(5,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ac43929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples\n",
      "Epoch 1/50\n",
      "132/132 [==============================] - 7s 55ms/sample - loss: 1.5874 - accuracy: 0.2879\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 400us/sample - loss: 1.4870 - accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 408us/sample - loss: 1.4251 - accuracy: 0.3939\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 453us/sample - loss: 1.3445 - accuracy: 0.4621\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 665us/sample - loss: 1.2353 - accuracy: 0.5227\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 680us/sample - loss: 1.0528 - accuracy: 0.6894\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 650us/sample - loss: 0.9427 - accuracy: 0.6515\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 786us/sample - loss: 0.8463 - accuracy: 0.6894\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 597us/sample - loss: 0.8074 - accuracy: 0.6742\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 461us/sample - loss: 0.6962 - accuracy: 0.7424\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 378us/sample - loss: 0.5516 - accuracy: 0.7955\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 363us/sample - loss: 0.5948 - accuracy: 0.8182\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 302us/sample - loss: 0.4965 - accuracy: 0.7879\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 431us/sample - loss: 0.3803 - accuracy: 0.8712\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 453us/sample - loss: 0.3907 - accuracy: 0.8561\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 710us/sample - loss: 0.3795 - accuracy: 0.8485\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 839us/sample - loss: 0.3748 - accuracy: 0.8939\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 657us/sample - loss: 0.4015 - accuracy: 0.8788\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 665us/sample - loss: 0.3254 - accuracy: 0.8939\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 589us/sample - loss: 0.3366 - accuracy: 0.9015\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 416us/sample - loss: 0.3009 - accuracy: 0.8864\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 453us/sample - loss: 0.2422 - accuracy: 0.9242\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 438us/sample - loss: 0.2300 - accuracy: 0.9318\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 348us/sample - loss: 0.1642 - accuracy: 0.9697\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 438us/sample - loss: 0.1499 - accuracy: 0.9545\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 468us/sample - loss: 0.2409 - accuracy: 0.9318\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 650us/sample - loss: 0.2225 - accuracy: 0.9318\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 892us/sample - loss: 0.2459 - accuracy: 0.9091\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 854us/sample - loss: 0.3586 - accuracy: 0.8939\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 718us/sample - loss: 0.2738 - accuracy: 0.9167\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 499us/sample - loss: 0.2939 - accuracy: 0.8939\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 400us/sample - loss: 0.2586 - accuracy: 0.9015\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 370us/sample - loss: 0.2264 - accuracy: 0.9242\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 385us/sample - loss: 0.1964 - accuracy: 0.9394\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 400us/sample - loss: 0.1666 - accuracy: 0.9394\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 514us/sample - loss: 0.1423 - accuracy: 0.9697\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 695us/sample - loss: 0.1179 - accuracy: 0.9697\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 952us/sample - loss: 0.1236 - accuracy: 0.9697\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 778us/sample - loss: 0.0870 - accuracy: 0.9697\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 982us/sample - loss: 0.0646 - accuracy: 0.9773\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 688us/sample - loss: 0.0603 - accuracy: 0.9924\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 446us/sample - loss: 0.0854 - accuracy: 0.9621\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 343us/sample - loss: 0.0562 - accuracy: 0.9773\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 380us/sample - loss: 0.0648 - accuracy: 0.9924\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 453us/sample - loss: 0.0406 - accuracy: 0.9924\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 536us/sample - loss: 0.0412 - accuracy: 0.9848\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 491us/sample - loss: 0.0390 - accuracy: 0.9848\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 876us/sample - loss: 0.0308 - accuracy: 0.9924\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 861us/sample - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 997us/sample - loss: 0.0586 - accuracy: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x225c0150248>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Now , the training finally begins.\n",
    "model.fit(\n",
    "    x=x_final, y=y_final.T, batch_size=32, epochs=50, verbose=1,\n",
    "    shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10752b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 23ms/sample - loss: 0.3716 - accuracy: 0.8750\n",
      "\n",
      "Test accuracy =  0.875\n"
     ]
    }
   ],
   "source": [
    "x_test_final = sentences_to_indicies(x_test, word_to_index, max_len)\n",
    "y_test_final = labels_to_oh(y_test)\n",
    "loss, acc = model.evaluate(x_test_final, y_test_final.T)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6c1ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojifier_model2( sentence,mod=model):\n",
    "    test=sentences_to_indicies(sentence, word_to_index, max_len)\n",
    "    pred=np.argmax( mod.predict(test) )\n",
    "    #print(pred)\n",
    "    print(*sentence, \"--->\" , label_to_emoji(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c3428d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am in good mood ---> üòÑ\n"
     ]
    }
   ],
   "source": [
    "r=\"i am in good mood\"  #feel free to play with this.\n",
    "emojifier_model2(np.array([r]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "822eaac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am not in good mood ---> üòû\n"
     ]
    }
   ],
   "source": [
    "r=\"i am not in good mood\"  \n",
    "emojifier_model2(np.array([r]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d3858",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f75a1a",
   "metadata": {},
   "source": [
    "### Model 1: \n",
    "------------\n",
    "Training set:\n",
    "Accuracy: 0.9848484848484849\n",
    "------------\n",
    "Test set:\n",
    "Accuracy: 0.8928571428571429\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf3cb8",
   "metadata": {},
   "source": [
    "### Model 2: \n",
    "------------\n",
    "Training set:\n",
    "Accuracy: 0.9773\n",
    "------------\n",
    "Test set:\n",
    "Accuracy: 0.8750\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536db12",
   "metadata": {},
   "source": [
    "Still, Model 2 is better than Model 1 because word ordering is taken in account in Model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51e09b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (mission)",
   "language": "python",
   "name": "mission"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
